<!DOCTYPE html>
<html>
<head>
<title> Assingment no.4 </title>
</head>
<body bgcolor="pink">
<h1> Internal link </h1>
<a href="#Introduction">Introduction </a><br>
<a href="#Lifecycle"> The Data Science Lifecycle </a><br>
<a href="#Data Scientist"> What is Data Scientist</a><br>
<br>


<a name="Introduction"> Introduction </a>
<p>Data Science is a combination of multiple 
disciplines that uses statistics,data analysis,
and machine learning to analyze data and 
 to extract knowledge and insights from it.
Data Science is about data gathering, analysis and decision-making.
Data Science is about finding patterns in data, through analysis, 
and make future predictionsData science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data. These insights can be used to guide decision making and strategic planning.

The accelerating volume of data sources, and subsequently data, has made data science is one of the fastest growing field across every industry. As a result, it is no surprise that the role of the data scientist was dubbed the “sexiest job of the 21st century” by Harvard Business Review (link resides outside ibm.com). Organizations are increasingly reliant on them to interpret data and provide actionable recommendations to improve business outcomes.

The data science lifecycle involves various roles, tools, and processes, which enables analysts to glean actionable insights. Typically, a data science project undergoes the following stages:

Data ingestion: The lifecycle begins with the data collection—both raw structured and unstructured data from all relevant sources using a variety of methods. These methods can include manual entry, web scraping, and real-time streaming data from systems and devices. Data sources can include structured data, such as customer data, along with unstructured data like log files, video, audio, pictures, the Internet of Things (IoT), social media, and more.
Data storage and data processing: Since data can have different formats and structures, companies need to consider different storage systems based on the type of data that needs to be captured. Data management teams help to set standards around data storage and structure, which facilitate workflows around analytics, machine learning and deep learning models. This stage includes cleaning data, deduplicating, transforming and combining the data using ETL (extract, transform, load) jobs or other data integration technologies. This data preparation is essential for promoting data quality before loading into a data warehouse, data lake, or other repository.
Data analysis: Here, data scientists conduct an exploratory data analysis to examine biases, patterns, ranges, and distributions of values within the data. This data analytics exploration drives hypothesis generation for a/b testing. It also allows analysts to determine the data’s relevance for use within modeling efforts for predictive analytics, machine learning, and/or deep learning. Depending on a model’s accuracy, organizations can become reliant on these insights for business decision making, allowing them to drive more scalability.
Communicate: Finally, insights are presented as reports and other data visualizations that make the insights—and their impact on business—easier for business analysts and other decision-makers to understand. A data science programming language such as R or Python includes components for generating visualizations; alternately, data scientists can use dedicated visualization tools.
Ebook
Data science and MLOps for data leaders
Use this ebook to align with other leaders on the 3 key goals of MLOps and trustworthy AI: trust in data, trust in models and trust in processes.

Related content
Register for the ebook on generative AI

Begin your journey to AI
Learn how to scale AI 
Explore the AI Academy 
Data science versus data scientist
Data science is considered a discipline, while data scientists are the practitioners within that field. Data scientists are not necessarily directly responsible for all the processes involved in the data science lifecycle. For example, data pipelines are typically handled by data engineers—but the data scientist may make recommendations about what sort of data is useful or required. While data scientists can build machine learning models, scaling these efforts at a larger level requires more software engineering skills to optimize a program to run more quickly. As a result, it’s common for a data scientist to partner with machine learning engineers to scale machine learning models.

Data scientist responsibilities can commonly overlap with a data analyst, particularly with exploratory data analysis and data visualization. However, a data scientist’s skillset is typically broader than the average data analyst. Comparatively speaking, data scientist leverage common programming languages, such as R and Python, to conduct more statistical inference and data visualization.

To perform these tasks, data scientists require computer science and pure science skills beyond those of a typical business analyst or data analyst. The data scientist must also understand the specifics of the business, such as automobile manufacturing, eCommerce, or healthcare.

In short, a data scientist must be able to:

Know enough about the business to ask pertinent questions and identify business pain points.
Apply statistics and computer science, along with business acumen, to data analysis.
Use a wide range of tools and techniques for preparing and extracting data—everything from databases and SQL to data mining to data integration methods.
Extract insights from big data using predictive analytics and artificial intelligence (AI), including machine learning models, natural language processing, and deep learning.
Write programs that automate data processing and calculations.
Tell—and illustrate—stories that clearly convey the meaning of results to decision-makers and stakeholders at every level of technical understanding.
Explain how the results can be used to solve business problems.
Collaborate with other data science team members, such as data and business analysts, IT architects, data engineers, and application developers.
These skills are in high demand, and as a result, many individuals that are breaking into a data science career, explore a variety of data science programs, such as certification programs, data science courses, and degree programs offered by educational institutions.

Now available: watsonx.ai
The all new enterprise studio that brings together traditional machine learning along with new generative AI capabilities powered by foundation models.

Try watsonx.ai 
Featured products
Watson Studio

IBM Cloud Pak for Data

Data science versus business intelligence
It may be easy to confuse the terms “data science” and “business intelligence” (BI) because they both relate to an organization’s data and analysis of that data, but they do differ in focus.

Business intelligence (BI) is typically an umbrella term for the technology that enables data preparation, data mining, data management, and data visualization. Business intelligence tools and processes allow end users to identify actionable information from raw data, facilitating data-driven decision-making within organizations across various industries. While data science tools overlap in much of this regard, business intelligence focuses more on data from the past, and the insights from BI tools are more descriptive in nature. It uses data to understand what happened before to inform a course of action. BI is geared toward static (unchanging) data that is usually structured. While data science uses descriptive data, it typically utilizes it to determine predictive variables, which are then used to categorize data or to make forecasts.

Data science and BI are not mutually exclusive—digitally savvy organizations use both to fully understand and extract value from their data.

Data science tools
Data scientists rely on popular programming languages to conduct exploratory data analysis and statistical regression. These open source tools support pre-built statistical modeling, machine learning, and graphics capabilities. These languages include the following (read more at "Python vs. R: What's the Difference?"):

R Studio: An open source programming language and environment for developing statistical computing and graphics.
Python: It is a dynamic and flexible programming language. The Python includes numerous libraries, such as NumPy, Pandas, Matplotlib, for analyzing data quickly.
To facilitate sharing code and other information, data scientists may use GitHub and Jupyter notebooks.

Some data scientists may prefer a user interface, and two common enterprise tools for statistical analysis include:

SAS: A comprehensive tool suite, including visualizations and interactive dashboards, for analyzing, reporting, data mining, and predictive modeling.
IBM SPSS: Offers advanced statistical analysis, a large library of machine learning algorithms, text analysis, open source extensibility, integration with big data, and seamless deployment into applications.
Data scientists also gain proficiency in using big data processing platforms, such as Apache Spark, the open source framework Apache Hadoop, and NoSQL databases. They are also skilled with a wide range of data visualization tools, including simple graphics tools included with business presentation and spreadsheet applications (like Microsoft Excel), built-for-purpose commercial visualization tools like Tableau and IBM Cognos, and open source tools like D3.js (a JavaScript library for creating interactive data visualizations) and RAW Graphs. For building machine learning models, data scientists frequently turn to several frameworks like PyTorch, TensorFlow, MXNet, and Spark MLib.

Given the steep learning curve in data science, many companies are seeking to accelerate their return on investment for AI projects; they often struggle to hire the talent needed to realize data science project’s full potential. To address this gap, they are turning to multipersona data science and machine learning (DSML) platforms, giving rise to the role of “citizen data scientist.”

Multipersona DSML platforms use automation, self-service portals, and low-code/no-code user interfaces so that people with little or no background in digital technology or expert data science can create business value using data science and machine learning. These platforms also support expert data scientists by also offering a more technical interface. Using a multipersona DSML platform encourages collaboration across the enterprise.

Data science and cloud computing
Cloud computing scales data science by providing access to additional processing power, storage, and other tools required for data science projects.

Since data science frequently leverages large data sets, tools that can scale with the size of the data is incredibly important, particularly for time-sensitive projects. Cloud storage solutions, such as data lakes, provide access to storage infrastructure, which are capable of ingesting and processing large volumes of data with ease. These storage systems provide flexibility to end users, allowing them to spin up large clusters as needed. They can also add incremental compute nodes to expedite data processing jobs, allowing the business to make short-term tradeoffs for a larger long-term outcome. Cloud platforms typically have different pricing models, such a per-use or subscriptions, to meet the needs of their end user—whether they are a large enterprise or a small startup.

Open source technologies are widely used in data science tool sets. When they’re hosted in the cloud, teams don’t need to install, configure, maintain, or update them locally. Several cloud providers, including IBM Cloud®, also offer prepackaged tool kits that enable data scientists to build models without coding, further democratizing access to technology innovations and data insights. 

Data science use cases
Enterprises can unlock numerous benefits from data science. Common use cases include process optimization through intelligent automation and enhanced targeting and personalization to improve the customer experience (CX). However, more specific examples include:

Here are a few representative use cases for data science and artificial intelligence:

An international bank delivers faster loan services with a mobile app using machine learning-powered credit risk models and a hybrid cloud computing architecture that is both powerful and secure.
An electronics firm is developing ultra-powerful 3D-printed sensors to guide tomorrow’s driverless vehicles. The solution relies on data science and analytics tools to enhance its real-time object detection capabilities.
A robotic process automation (RPA) solution provider developed a cognitive business process mining solution that reduces incident handling times between 15% and 95% for its client companies. The solution is trained to understand the content and sentiment of customer emails, directing service teams to prioritize those that are most relevant and urgent.
A digital media technology company created an audience analytics platform that enables its clients to see what’s engaging TV audiences as they’re offered a growing range of digital channels. The solution employs deep analytics and machine learning to gather real-time insights into viewer behavior.
An urban police department created statistical incident analysis tools (link resides outside ibm.com) to help officers understand when and where to deploy resources in order to prevent crime. The data-driven solution creates reports and dashboards to augment situational awareness for field officers.
Shanghai Changjiang Science and Technology Development used IBM® Watson® technology to build an AI-based medical assessment platform that can analyze existing medical records to categorize patients based on their risk of experiencing a stroke and that can predict the success rate of different treatment plans.
Related solutions
watsonx.ai
Experiment with foundation models and build machine learning models automatically in our next-generation studio for AI builders.

Explore watsonx.ai 
View the interactive demo 
IBM Watson® Studio
Synchronize DevOps and ModelOps. Build and scale AI models with your cloud-native apps across virtually any cloud.

Explore IBM Watson® Studio today 
Explainable AI
Increase AI interpretability. Assess and mitigate AI risks. Deploy AI with trust and confidence.

Learn more about Explainable AI 
AutoAI
Build and train high-quality predictive models quickly. Simplify AI lifecycle management.

Explore AutoAI 
Resources
Autostrade per l’Italia
Autostrade per l’Italia implemented several IBM solutions for a complete digital transformation to improve how it monitors and maintains its vast array of infrastructure assets.

MANA Community
MANA Community teamed with IBM Garage to build an AI platform to mine huge volumes of environmental data volumes from multiple digital channels and thousands of sources.

Article
Architectural thinking in the Wild West of data science
Having a complete freedom in choice of programming languages, tools and frameworks improves creative thinking and evolvement.

Take the next step
Scale AI workloads for all your data, anywhere, with IBM watsonx.data, a fit-for-purpose data store built 
 on an open data lakehouse architecture.

Explore watsonx.data
Book a live demo
United States — English
About IBM
Overview
Annual report
Corporate social responsibility
Diversity & inclusion
Financing
Investor
Newsroom
Security, privacy & trust
Senior leadership
Careers with IBM
IBM Research
Website
Blog
Publications
Collaborate with us
Topics
Artificial intelligence
Machine learning
Conversational AI
AI governance
CSRD
Cybersecurity
Predictive analytics
Quantum computing
Partners
Our strategic partners
Find a partner
Become a partner - Partner Plus
Partner Plus log in
Engage with IBM
IBM TechXChange Community
LinkedIn
X
Instagram
YouTube
Subscription Center
Participate in user experience research
Contact IBM
Privacy
Terms of use
Accessibility
Cookie Preferences

close icon
About cookies on this site</p>
<br>
<br>


<a name ="Lifecycle"> The Data SCience Lifecycle </a>
<p>Data Science Lifecycle revolves around the use of machine learning
 and different analytical strategies to produce insights and predictions 
 from information in order to acquire a commercial enterprise objective. 
 The complete method includes a number of steps like data cleaning,
 preparation, modelling, model evaluation, etc. It is a lengthy procedure 
 and may additionally take quite a few months to complete.
 So, it is very essential to have a generic structure to observe
 for each and every hassle at hand. The globally mentioned structure in fixing any
 analytical problem is referred to as a Cross Industry 
 Standard Process for Data Mining or CRISP-DM framework.
 A data science lifecycle is defined as the iterative set 
 of data science steps required to deliver a project or analysis.
 There are no one-size-fits that define data science projects.
 Hence you need to determine the one that best fits your business
 requirements. Each step in the lifecycle should be performed carefully.
 Any improper execution will affect the following step, ultimately 
 impacting the entire process.Data Science is key to achieving a perfect 
 data science process here. The main aim is to build a framework and solutions
 to store data. Since every data science project and the team is unique, every 
 specific data science lifecycle is different.Like any other good business lifecycle,
 the data science lifecycle also starts with ‘why?’ Identifying problems is one of
 the major steps necessary in the data science process to find a clear objective
 around which all the following steps will be formulated. In short, it is important
 to understand the business objective early since it will decide the final goal
 of your analysis. This phase should examine the trends of business, analyse
 case studies of similar analysis, and study the industry’s domain. The team 
 will assess in-house resources, infrastructure, total time, and technology needs.
 Once these aspects are all identified and evaluated, they will prepare an initial
 hypothesis to resolve the business challenges following the current scenario.
 The phase should –Now, we are at the final stage of the lifecycle of data science.
 After a rigorous evaluation process, the model is finally prepared to be deployed 
 in the desired format and preferred channel. Remember, there is no value for the
 machine learning model until it’s deployed to production. Hence machine learning
 models have to be recorded before the deployment process. In general, these models
 are integrated and coupled with products and applications. 
The stage of Model deployment involves the creation of a delivery mechanism 
required to get the mode out in the market among the users or to another system.
 Machine learning models are also deployed on devices and gaining adoption and popularity 
 in the field of computing. From simple model output in a Tableau Dashboard to a complex as scaling 
  it to cloud in front of millions of users, this step is distinct for different projects.

</p>
<br>
<br>

<a name="Data Scientist"> What is data Scientist </a>
<p>A data scientist is an analytics professional who is responsible for collecting,
 analyzing and interpreting data to help drive decision-making in an organization.
 The data scientist role combines elements of several traditional and technical jobs,
 including mathematician, scientist, statistician and computer programmer. 
 It involves the use of advanced analytics techniques, such as machine learning
 and predictive modeling, along with the application of scientific principles.
 As part of data science initiatives, data scientists often must work with 
 large amounts of data to develop and test hypotheses, make inferences and 
 analyze things such as customer and market trends, financial risks, cybersecurity
 threats, stock trades, equipment maintenance needs and medical conditions.In businesses,
 data scientists typically mine data for information that can be used to predict 
 customer behavior, identify new revenue opportunities, detect fraudulent transactions
 and meet other business needs. They also do valuable analytics work for healthcare
 providers, academic institutions, government agencies, sports teams and other types
 of organizations.Data scientist was first used as a job title in 2008, simultaneously 
 at Facebook and LinkedIn; four years later, a Harvard Business Review article famously
 called it "the sexiest job of the 21st century." The demand for data science skills has
 grown significantly over the years, as companies look to glean useful information from
 increasing volumes of big data and take advantage of artificial intelligence (AI) 
 and machine learning technologies to enable new types of analytics applications.
 Data collection is the next stage in the data science lifecycle to gather raw data
 from relevant sources. The data captured can be either in structured or unstructured 
 form. The methods of collecting the data might come from – logs from websites, social
 media data, data from online repositories, and even data streamed from online sources 
 via APIs, web scraping or data that could be present in excel or any other source.
The person performing the task should know the difference between various data sets
 available and the data investment strategy of an organisation. A major challenge faced
 by professionals in this step is tracking where each data comes from and whether it is up-to-date.
 It is important to keep track of this information throughout the entire lifecycle of a data science
 project as it might help test hypotheses or run any other updated experiments.n
 this phase, data scientists analyse the data collected for biases, patterns, ranges,
 and distribution of values. It is done to determine the sustainability of the databases 
 and predicts their usage in regression, machine learning and deep learning algorithms. 
 The phase also involves the introspection of different types of data, including nominal, 
 numerical, and categorical dataData visualisation is also done to highlight the critical 
 trends and patterns of data, comprehended by simple bars and line charts. Simply put, data
 processing might be the most time-consuming but arguably the most critical phase in the entire
 life cycle of data analytics. The goodness of the model depends on this data processing stage.</p>
<br>
<br>



<h1> Telephone link </h1>
<p> To create a link  to a telephone number,use tel: inside href attributes: </p>
<p> <a href="tel":+91-1234567 ></a> <p>
<br>

<h1> Email linlk </h1>
<p> To create a link that opens in the user's email program  inside href atributes</p>
<p> <a href="mailto:"alicode9.3@gamil.com"> send email </a> </p>

</body>